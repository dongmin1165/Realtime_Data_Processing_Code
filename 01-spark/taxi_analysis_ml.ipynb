{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be00c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67adac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"5g\"\n",
    "spark = SparkSession.builder.appName(\"trip_count_by_zone_sql\")\\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY)\\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa431b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_files = \"/Users/keon/fastcampus/data-engineering/01-spark/data/trips/*\"\n",
    "zone_file = \"/Users/keon/fastcampus/data-engineering/01-spark/data/taxi+_zone_lookup.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25268510",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df = spark.read.csv(f\"file:///{trip_files}\", inferSchema = True, header = True)\n",
    "zones_df = spark.read.csv(f\"file:///{zone_file}\", inferSchema = True, header = True)\n",
    "# trip_data = spark.read.format(\"csv\").option(\"header\", \"true\").load(trip_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6dffc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips_df.printSchema()\n",
    "zones_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2255f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.createOrReplaceTempView(\"trips\")\n",
    "zones_df.createOrReplaceTempView(\"zones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bba6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT \n",
    "        t.VendorID as vendor_id,\n",
    "        TO_DATE(t.tpep_pickup_datetime) as pickup_date,\n",
    "        TO_DATE(t.tpep_dropoff_datetime) as dropoff_date,\n",
    "        HOUR(t.tpep_pickup_datetime) as pickup_time,\n",
    "        HOUR(t.tpep_dropoff_datetime) as dropoff_time,\n",
    "        DATE_FORMAT(TO_DATE(t.tpep_pickup_datetime), 'EEEE') AS day_of_week,\n",
    "        t.trip_distance,\n",
    "        t.RatecodeID as rate_code,\n",
    "        t.store_and_fwd_flag,\n",
    "        t.fare_amount,\n",
    "        t.tip_amount,\n",
    "        t.tolls_amount,\n",
    "        t.total_amount,\n",
    "        t.payment_type,\n",
    "        t.passenger_count,\n",
    "        t.PULocationID as pickup_location_id,\n",
    "        pz.Zone as pickup_zone,\n",
    "        pz.Borough as pickup_borough,\n",
    "        t.DOLocationID as dropoff_location_id,\n",
    "        dz.Zone as dropoff_zone,\n",
    "        dz.Borough as dropoff_borough,\n",
    "        ROUND((\n",
    "            UNIX_TIMESTAMP(t.tpep_dropoff_datetime) - \n",
    "            UNIX_TIMESTAMP(t.tpep_pickup_datetime)) / 60,\n",
    "            2) as duration_min\n",
    "    FROM \n",
    "        trips t\n",
    "    LEFT JOIN\n",
    "        zones pz\n",
    "        ON\n",
    "            t.PULocationID = pz.LocationID\n",
    "    LEFT JOIN\n",
    "        zones dz\n",
    "        ON\n",
    "            t.DOLocationID = dz.LocationID\n",
    "\"\"\"\n",
    "combo_df = spark.sql(query)\n",
    "combo_df.createOrReplaceTempView(\"combo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a69592e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|trip_distance|duration_min|\n",
      "+-------------+------------+\n",
      "|          2.1|        6.03|\n",
      "|          0.2|        0.98|\n",
      "|         14.7|        27.6|\n",
      "|         10.6|       15.22|\n",
      "|         4.94|       16.53|\n",
      "|          1.6|        8.02|\n",
      "|          4.1|        17.0|\n",
      "|          5.7|       18.08|\n",
      "|          9.1|       20.95|\n",
      "|          2.7|       13.57|\n",
      "|         6.11|       22.25|\n",
      "|         1.21|        7.15|\n",
      "|          7.4|        22.2|\n",
      "|          1.7|        7.77|\n",
      "|         0.81|        2.22|\n",
      "|         1.01|        4.12|\n",
      "|         0.73|        4.98|\n",
      "|         1.17|        4.95|\n",
      "|         0.78|        3.62|\n",
      "|         1.66|        8.57|\n",
      "+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combo_df.select([\"trip_distance\", \"duration_min\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b9a2d",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10f99b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    pickup_time,\n",
    "    day_of_week,\n",
    "    trip_distance,\n",
    "    passenger_count,\n",
    "    pickup_location_id,\n",
    "    pickup_borough,\n",
    "    dropoff_location_id,\n",
    "    dropoff_borough,\n",
    "    duration_min\n",
    "FROM\n",
    "    combo c\n",
    "WHERE \n",
    "    c.total_amount < 10000\n",
    "    AND c.total_amount > 0\n",
    "    AND c.trip_distance < 1000\n",
    "    AND c.passenger_count < 100\n",
    "    AND c.pickup_date >= '2021-01-01'\n",
    "    AND c.pickup_date < '2021-08-01'\n",
    "    AND c.trip_distance > 0\n",
    "\"\"\"\n",
    "model_df = spark.sql(query)\n",
    "model_df.createOrReplaceTempView(\"model_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e7d9e",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a97b45de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set count:  11133918\n",
      "Test set count: 2784353\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = model_df.randomSplit([0.8, 0.2], seed=2019)\n",
    "toy_df = train_df.sample(False, .1, seed=261)\n",
    "print(\"Train set count: \", train_df.count())\n",
    "print(\"Test set count:\", test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b60b48f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼 기반 포멧 parquet로 저장.. 압축률이 좋고 disk io가 적다 컬럼별로 적절한 인코딩이 가능\n",
    "data_dir = \"/Users/keon/fastcampus/data-engineering/01-spark/data\"\n",
    "train_df.write.format(\"parquet\").save(f\"{data_dir}/train/\")\n",
    "test_df.write.format(\"parquet\").save(f\"{data_dir}/test/\")\n",
    "toy_df.write.format(\"parquet\").save(f\"{data_dir}/toy/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38256f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다시 읽어오기\n",
    "train_df = spark.read.parquet(f\"{data_dir}/train/\")\n",
    "test_df = spark.read.parquet(f\"{data_dir}/test/\")\n",
    "toy_df = spark.read.parquet(f\"{data_dir}/toy/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e850c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "# list of categorical features\n",
    "cat_feats = [\n",
    "    \"pickup_borough\",\n",
    "    \"dropoff_borough\",\n",
    "    \"pickup_location_id\",\n",
    "    \"dropoff_location_id\",\n",
    "    \"day_of_week\",\n",
    "    \"pickup_time\",\n",
    "]\n",
    "\n",
    "# Initiate pipeline stages\n",
    "stages = []\n",
    "\n",
    "# Add one hot encoding transformation of categorical variables into the pipeline stage\n",
    "for c in cat_feats:\n",
    "    # cast each record in in categorical column c to an index\n",
    "    cat_indexer = StringIndexer(inputCol=c, outputCol = c + \"_idx\").setHandleInvalid(\"keep\")\n",
    "    # one hot encode \n",
    "    onehot_encoder = OneHotEncoder(inputCols=[cat_indexer.getOutputCol()], outputCols=[c + \"_onehot\"])\n",
    "    stages += [cat_indexer, onehot_encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46656628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Normalization\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# Numerical features to be scaled\n",
    "num_feats = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\"\n",
    "]\n",
    "\n",
    "# Add scaling transformation into the pipeline stages\n",
    "for n in num_feats:\n",
    "    num_assembler = VectorAssembler(inputCols=[n], outputCol=n+\"_vector\")\n",
    "    num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol=n+\"_scaled\")\n",
    "    stages += [num_assembler, num_scaler]\n",
    "    \n",
    "# Add feature vector creation into the pipeline stages\n",
    "assembler_inputs = [c + \"_onehot\" for c in cat_feats] + [n + \"_scaled\" for n in num_feats]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"feature_vector\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2da6170",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c06559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c0c9dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add model fitting into the pipeline\n",
    "model = LinearRegression(maxIter=50, \n",
    "                         solver=\"normal\", \n",
    "                         labelCol=\"duration_min\",\n",
    "                         featuresCol=\"feature_vector\")\n",
    "\n",
    "cv_stages = stages + [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdb3eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct pipeline using the set of stages defined\n",
    "cv_pipeline =  Pipeline(stages=cv_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59c68e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a grid of parameters to search over\n",
    "parameter_grid = ParamGridBuilder() \\\n",
    "                 .addGrid(model.elasticNetParam, [0.1, 0.2, 0.3, 0.4]) \\\n",
    "                 .addGrid(model.regParam, [0.01, 0.02, 0.03, 0.04]) \\\n",
    "                 .build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7c750bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cross validation for hyperparameter tuning\n",
    "cross_val = CrossValidator(estimator=cv_pipeline,\n",
    "                           estimatorParamMaps=parameter_grid,\n",
    "                           evaluator=RegressionEvaluator(labelCol=\"duration_min\"), \n",
    "                           numFolds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b73fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = cross_val.fit(toy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b1715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = cv_model.bestModel.stages[-1]._java_obj.getElasticNetParam()\n",
    "reg_param = cv_model.bestModel.stages[-1]._java_obj.getRegParam()\n",
    "print(\"alpha\", alpha)\n",
    "print(\"reg_param\", reg_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c3a1d4",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4bbcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add model into the stages\n",
    "transform_stages = stages\n",
    "\n",
    "# Construct pipeline using the set of stages defined\n",
    "transform_pipeline =  Pipeline(stages=transform_stages)\n",
    "\n",
    "# Fit the transformer\n",
    "fitted_transformer = transform_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da73f146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the train data\n",
    "transformed_train_df = fitted_transformer.transform(train_df)\n",
    "transformed_train_df = transformed_train_df.select(\"duration_min\", \"feature_vector\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a95936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters\n",
    "alpha = cv_model.bestModel.stages[-1]._java_obj.getElasticNetParam()\n",
    "reg_param = cv_model.bestModel.stages[-1]._java_obj.getRegParam()\n",
    "\n",
    "model = LinearRegression(maxIter=100, \n",
    "                         solver=\"normal\", \n",
    "                         labelCol=\"duration_min\",\n",
    "                         featuresCol=\"feature_vector\", \n",
    "#                          elasticNetParam=alpha, \n",
    "#                          regParam=reg_param,\n",
    "                        )\n",
    "\n",
    "# fit the model against train_df\n",
    "fitted_model = model.fit(transformed_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345eaab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_df = fitted_transformer.transform(test_df)\n",
    "transformed_test_df = transformed_test_df.select(\"duration_min\", \"feature_vector\").cache()\n",
    "transformed_test_df = fitted_model.transform(transformed_test_df).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"duration_min\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "rmse = evaluator.evaluate(transformed_test_df)\n",
    "print(\"RMSE of Prediction on test set:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0252814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ab2df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_df.select(\"prediction\").describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2649a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "\n",
    "data = [0, \"Friday\", 0.01, 1, 186, \"Manhattan\", 186, \"Manhattan\", 0.35]\n",
    "schema = ['pickup_time','day_of_week','trip_distance','passenger_count','pickup_location_id','pickup_borough','dropoff_location_id','dropoff_borough','duration_min']\n",
    "data_df = spark.createDataFrame(distance_list, schema)\n",
    "vdata_df = assembler.transform(data_df)\n",
    "model.transform(vdata_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4735afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35e030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
